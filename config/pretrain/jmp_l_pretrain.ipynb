{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_draft_config=False id='udmlef7l' name=None project=None tags=[] notes=[] debug=False environment=EnvironmentConfig(is_draft_config=False, cwd=None, python_executable=None, python_path=None, python_version=None, config=None, model=None, data=None, slurm=None, log_dir=None, seed=None, seed_workers=None, sweep_id=None, sweep_config=None) trainer=TrainerConfig(is_draft_config=False, python_logging=PythonLogging(is_draft_config=False, log_level=None, rich=True, rich_tracebacks=True, lovely_tensors=True, lovely_numpy=False), logging=LoggingConfig(is_draft_config=False, enabled=True, log_lr=True, log_epoch=True, wandb=WandbLoggingConfig(is_draft_config=False, enabled=True, log_model=False, watch=WandbWatchConfig(is_draft_config=False, enabled=True, log=None, log_graph=True, log_freq=100)), csv=CSVLoggingConfig(is_draft_config=False, enabled=True), tensorboard=TensorboardLoggingConfig(is_draft_config=False, enabled=False)), optimizer=OptimizerConfig(is_draft_config=False, grad_finite_checks=False, grad_none_checks=False, log_grad_norm=True, log_grad_norm_per_param=False, log_param_norm=False, log_param_norm_per_param=False, gradient_clipping=GradientClippingConfig(is_draft_config=False, enabled=True, value=1.0, algorithm='norm'), gradient_skipping=None), seed=0, seed_workers=False, default_ckpt_path=None, auto_wrap_trainer=True, auto_set_default_root_dir=True, auto_set_loggers=True, checkpoint_last_by_default=True, on_exception_checkpoint=True, auto_add_trainer_finalizer=True, enable_logger_validation=True, supports_skip_batch_exception=False, supports_shared_parameters=True, supports_parameter_hooks=False, log_batch_info_on_error=False, reduce_lr_on_plateau_sanity_checks='error', additional_trainer_kwargs={}, additional_env_vars={}, set_nccl_optimal_params=False, set_float32_matmul_precision='medium', accelerator='auto', strategy='auto', devices='auto', num_nodes='auto', precision='16-mixed', logger=None, fast_dev_run=False, max_epochs=None, min_epochs=None, max_steps=-1, min_steps=None, max_time=None, limit_train_batches=None, limit_val_batches=None, limit_test_batches=None, limit_predict_batches=None, overfit_batches=0.0, val_check_interval=None, check_val_every_n_epoch=1, num_sanity_val_steps=None, log_every_n_steps=50, enable_checkpointing=None, enable_progress_bar=None, enable_model_summary=None, accumulate_grad_batches=1, deterministic=None, benchmark=None, inference_mode=True, use_distributed_sampler=False, profiler=None, detect_anomaly=False, barebones=False, plugins=None, sync_batchnorm=False, reload_dataloaders_every_n_epochs=0, default_root_dir=None) runner=RunnerConfig(is_draft_config=False, auto_call_trainer_init_from_runner=True, save_output=None) meta={} optimizer=AdamWConfig(is_draft_config=False, name='adamw', lr=0.0003, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-08, amsgrad=False) lr_scheduler=LinearWarmupCosineAnnealingSchedulerConfig(is_draft_config=False, name='linear_warmup_cosine_annealing', warmup_steps=2000, max_steps=None, max_epochs=2, warmup_start_lr_factor=0.2, min_lr_factor=0.1, last_step=-1) activation='scaled_silu' dropout=None edge_dropout=0.1 embedding=EmbeddingConfig(is_draft_config=False, num_elements=120, embedding_size=256) backbone=BackboneConfig(is_draft_config=False, num_targets=1, num_spherical=7, num_radial=128, num_blocks=4, emb_size_atom=256, emb_size_edge=512, emb_size_trip_in=64, emb_size_trip_out=64, emb_size_quad_in=32, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_sbf=32, num_before_skip=2, num_after_skip=2, num_concat=1, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, num_global_out_layers=2, regress_forces=True, regress_energy=True, direct_forces=True, use_pbc=True, scale_backprop_forces=False, rbf={'name': 'gaussian'}, rbf_spherical=None, envelope={'name': 'polynomial', 'exponent': 5}, cbf={'name': 'spherical_harmonics'}, sbf={'name': 'legendre_outer'}, extensive=True, forces_coupled=False, activation='scaled_silu', quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, scale_basis=False, qint_tags=[1, 2], num_elements=120, otf_graph=False, scale_file=None, absolute_rbf_cutoff=12.0, learnable_rbf=False, learnable_rbf_stds=False, unique_basis_per_layer=False, dropout=None, edge_dropout=0.1) output=OutputConfig(is_draft_config=False, num_mlps=5, output_init='HeOrthogonal') batch_size=4 eval_batch_size=None num_workers=0 pin_memory=True shuffle_train=True shuffle_val=False log_task_losses=True log_task_steps_and_epochs=True tasks=[TaskConfig(is_draft_config=False, name='oc20', train_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/datasets/s2ef/2M/train'), metadata_path=PosixPath('/datasets/s2ef/2M/train_metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), val_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/datasets/s2ef/all/val_id'), metadata_path=PosixPath('/datasets/s2ef/all/val_id_metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), node_energy_reduction='sum', additional_units=[], energy_loss_scale=1.0, force_loss_scale=73.0, normalization={'y': NormalizationConfig(is_draft_config=False, mean=0.0, std=24.901469505465872), 'force': NormalizationConfig(is_draft_config=False, mean=0.0, std=0.5111534595489502)}), TaskConfig(is_draft_config=False, name='oc22', train_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/train'), metadata_path=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/train/metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), val_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/val_id'), metadata_path=PosixPath('/shared/pre-training-datasets/oc22/s2ef-total/val_id/metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), node_energy_reduction='sum', additional_units=[], energy_loss_scale=1.0, force_loss_scale=80.0, normalization={'y': NormalizationConfig(is_draft_config=False, mean=0.0, std=25.229595396538468), 'force': NormalizationConfig(is_draft_config=False, mean=0.0, std=0.25678861141204834)}), TaskConfig(is_draft_config=False, name='ani1x', train_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/shared/pre-training-datasets/ani1x/train'), metadata_path=PosixPath('/shared/pre-training-datasets/ani1x/train/metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), val_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/shared/pre-training-datasets/ani1x/val'), metadata_path=PosixPath('/shared/pre-training-datasets/ani1x/val/metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), node_energy_reduction='sum', additional_units=[], energy_loss_scale=1.0, force_loss_scale=15.0, normalization={'y': NormalizationConfig(is_draft_config=False, mean=0.0, std=2.8700712783472118), 'force': NormalizationConfig(is_draft_config=False, mean=0.0, std=2.131422996520996)}), TaskConfig(is_draft_config=False, name='transition1x', train_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/shared/pre-training-datasets/trans1x/train'), metadata_path=PosixPath('/shared/pre-training-datasets/trans1x/train/metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), val_dataset=PretrainDatasetConfig(is_draft_config=False, sample_n=None, atom_ref=None, src=PosixPath('/shared/pre-training-datasets/trans1x/val'), metadata_path=PosixPath('/shared/pre-training-datasets/trans1x/val/metadata.npz'), total_energy=None, oc20_ref=None, lin_ref=None), node_energy_reduction='sum', additional_units=[], energy_loss_scale=1.0, force_loss_scale=14.0, normalization={'y': NormalizationConfig(is_draft_config=False, mean=0.0, std=1.787466168382901), 'force': NormalizationConfig(is_draft_config=False, mean=0.0, std=0.3591422140598297)})] mt_dataset=MTDatasetConfig(is_draft_config=False, balanced=None, strict=True, taskify_keys_graph=['y', 'y_scale', 'force_scale'], taskify_keys_node=['force'], taskify_use_onehot=True, sample_type='temperature', sample_temperature=2.0) exclude_keys=['id', 'fid', 'cell_offsets', 'edge_index', 'absolute_idx', 'target_pos', 'ref_energy', 'pbc', 'oc22', 'name'] train_on_free_atoms_only=False eval_on_free_atoms_only=True energy_loss_reduction='mean' force_loss_reduction='mean' structurewise_loss_reduction=True ema=EMAConfig(is_draft_config=False, decay=0.99, validate_original_weights=False, every_n_steps=1, cpu_offload=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/ll/src/ll/model/config.py:711: IdSeedWarning: BaseConfig._rng is None. The generated IDs will not be reproducible. To fix this, call BaseConfig.set_seed(...) before generating any IDs.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from jmppeft.tasks.pretrain import PretrainConfig, PretrainModel\n",
    "from jmppeft.tasks.pretrain.module import (\n",
    "    NormalizationConfig,\n",
    "    PretrainDatasetConfig,\n",
    "    TaskConfig,\n",
    ")\n",
    "from jmppeft.configs.pretrain.jmp_l import jmp_l_pt_config_\n",
    "\n",
    "\n",
    "# Let's make the config\n",
    "def jmp_l_config():\n",
    "    config = PretrainConfig.draft()\n",
    "    jmp_l_pt_config_(config)\n",
    "\n",
    "    # Set data config\n",
    "    config.batch_size = 4\n",
    "    config.num_workers = 0\n",
    "\n",
    "    # Set the tasks\n",
    "    config.tasks = [\n",
    "        TaskConfig(\n",
    "            name=\"oc20\",\n",
    "            train_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/datasets/s2ef/2M/train/\"),\n",
    "                metadata_path=Path(\"/datasets/s2ef/2M/train_metadata.npz\"),\n",
    "            ),\n",
    "            val_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/datasets/s2ef/all/val_id/\"),\n",
    "                metadata_path=Path(\"/datasets/s2ef/all/val_id_metadata.npz\"),\n",
    "            ),\n",
    "            energy_loss_scale=1.0,\n",
    "            force_loss_scale=73.0,\n",
    "            normalization={\n",
    "                \"y\": NormalizationConfig(mean=0.0, std=24.901469505465872),\n",
    "                \"force\": NormalizationConfig(mean=0.0, std=0.5111534595489502),\n",
    "            },\n",
    "        ),\n",
    "        TaskConfig(\n",
    "            name=\"oc22\",\n",
    "            train_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/shared/pre-training-datasets/oc22/s2ef-total/train/\"),\n",
    "            ),\n",
    "            val_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/shared/pre-training-datasets/oc22/s2ef-total/val_id/\"),\n",
    "            ),\n",
    "            energy_loss_scale=1.0,\n",
    "            force_loss_scale=80.0,\n",
    "            normalization={\n",
    "                \"y\": NormalizationConfig(mean=0.0, std=25.229595396538468),\n",
    "                \"force\": NormalizationConfig(mean=0.0, std=0.25678861141204834),\n",
    "            },\n",
    "        ),\n",
    "        TaskConfig(\n",
    "            name=\"ani1x\",\n",
    "            train_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/shared/pre-training-datasets/ani1x/train/\"),\n",
    "            ),\n",
    "            val_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/shared/pre-training-datasets/ani1x/val/\"),\n",
    "            ),\n",
    "            energy_loss_scale=1.0,\n",
    "            force_loss_scale=15.0,\n",
    "            normalization={\n",
    "                \"y\": NormalizationConfig(mean=0.0, std=2.8700712783472118),\n",
    "                \"force\": NormalizationConfig(mean=0.0, std=2.131422996520996),\n",
    "            },\n",
    "        ),\n",
    "        TaskConfig(\n",
    "            name=\"transition1x\",\n",
    "            train_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/shared/pre-training-datasets/trans1x/train/\"),\n",
    "            ),\n",
    "            val_dataset=PretrainDatasetConfig(\n",
    "                src=Path(\"/shared/pre-training-datasets/trans1x/val/\"),\n",
    "            ),\n",
    "            energy_loss_scale=1.0,\n",
    "            force_loss_scale=14.0,\n",
    "            normalization={\n",
    "                \"y\": NormalizationConfig(mean=0.0, std=1.787466168382901),\n",
    "                \"force\": NormalizationConfig(mean=0.0, std=0.3591422140598297),\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return config.finalize()\n",
    "\n",
    "\n",
    "config = jmp_l_config()\n",
    "print(config)\n",
    "\n",
    "configs: list[tuple[PretrainConfig, type[PretrainModel]]] = []\n",
    "configs.append((config, PretrainModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d150a045ace4327b22edca59579cc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fast dev run:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import rich. Falling back to default Python logging.\n",
      "CRITICAL:ll.trainer.trainer:Setting config.trainer.default_root_dir='/workspaces/repositories/jmp-peft/config/pretrain/lightning_logs/u4r9wrhn'.\n",
      "Seed set to 0\n",
      "CRITICAL:ll.util.seed:Set global seed to 0.\n",
      "CRITICAL:ll.runner:Auto-wrapping run in Trainer context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized arguments:  dict_keys(['is_draft_config', 'learnable_rbf', 'learnable_rbf_stds', 'unique_basis_per_layer', 'dropout', 'edge_dropout'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:ll.trainer.trainer:Disabling loggers because fast_dev_run is enabled.\n",
      "CRITICAL:ll.trainer.trainer:Setting num_nodes to 1 (no SLURM detected).\n",
      "CRITICAL:ll.trainer.trainer:LightningTrainer.__init__ with args=() and kwargs={'accelerator': 'auto', 'strategy': 'auto', 'devices': 'auto', 'num_nodes': 1, 'precision': '16-mixed', 'logger': None, 'fast_dev_run': 16, 'max_epochs': None, 'min_epochs': None, 'max_steps': -1, 'min_steps': None, 'max_time': None, 'limit_train_batches': None, 'limit_val_batches': None, 'limit_test_batches': None, 'limit_predict_batches': None, 'overfit_batches': 0.0, 'val_check_interval': None, 'check_val_every_n_epoch': 1, 'num_sanity_val_steps': None, 'log_every_n_steps': 50, 'enable_checkpointing': None, 'enable_progress_bar': None, 'enable_model_summary': None, 'accumulate_grad_batches': 1, 'deterministic': None, 'benchmark': None, 'inference_mode': True, 'use_distributed_sampler': False, 'detect_anomaly': False, 'barebones': False, 'plugins': [], 'sync_batchnorm': False, 'reload_dataloaders_every_n_epochs': 0, 'gradient_clip_algorithm': 'norm', 'gradient_clip_val': 1.0, 'default_root_dir': '/workspaces/repositories/jmp-peft/config/pretrain/lightning_logs/u4r9wrhn', 'callbacks': [<lightning.pytorch.callbacks.on_exception_checkpoint.OnExceptionCheckpoint object at 0x7f6a6b1d3710>]}.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 16 batch(es). Logging and checkpointing is suppressed.\n",
      "WARNING:ll.trainer.logging:Logger DummyLogger does not support run_id, ignoring.\n",
      "CRITICAL:ll.trainer.trainer:LightningTrainer log directory: None.\n",
      "/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:126: `.fit(ckpt_path=None)` was called without a model. The last model of the previous `fit` call will be used. You can pass `fit(ckpt_path='best')` to use the best model or `fit(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:186: .fit(ckpt_path=\"last\") is set, but there is no last checkpoint available. No checkpoint will be loaded. HINT: Set `ModelCheckpoint(..., save_last=True)`.\n",
      "WARNING:ll.model.modules.wandb:Could not find wandb logger or module to log\n",
      "CRITICAL:ll.model.base:Fast dev run detected, setting debug flag to True.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "CRITICAL:jmppeft.tasks.config:Optimizer: AdamW\n",
      "Optimizer kwargs: {}\n",
      "Base kwargs: {}\n",
      "Param groups: Param group 0:\n",
      "    Params: 405\n",
      "    Total param size: 44131328\n",
      "    Other kwargs: {'lr': 0.0003, 'amsgrad': False, 'weight_decay': 0.1, 'betas': (0.9, 0.95), 'eps': 1e-08, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "CRITICAL:jmppeft.tasks.pretrain.module:Setting max_steps=32 by default.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | embedding     | Embedding        | 30.7 K\n",
      "1 | backbone      | GemNetOCBackbone | 38.8 M\n",
      "2 | output        | Output           | 5.3 M \n",
      "3 | train_metrics | FMMetrics        | 0     \n",
      "4 | val_metrics   | FMMetrics        | 0     \n",
      "5 | task_steps    | TypedModuleDict  | 0     \n",
      "---------------------------------------------------\n",
      "44.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.1 M    Total params\n",
      "176.525   Total estimated model params size (MB)\n",
      "CRITICAL:jmppeft.modules.dataset.concat_dataset:Ignoring balancing because `ignore_balancing` is True in `MTSampledDataset.__init__`.\n",
      "/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685d848fa0094f408b2eff459cb1b7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:ll.trainer.trainer:Ran 1 finalizers for Trainer cleanup.\n",
      "Seed set to 0\n",
      "CRITICAL:ll.util.seed:Reset global seed.\n",
      "CRITICAL:ll.runner:Error in run with run_id='udmlef7l' (run_name=None): Type-check error whilst checking the parameters of radius_graph_pbc.\n",
      "The problem arose whilst typechecking parameter 'data'.\n",
      "Actual value: Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22')\n",
      "Expected type: <class 'torch_geometric.data.batch.Batch'>.\n",
      "----------------------\n",
      "Called with parameters: { 'data': Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22'),\n",
      "  'max_num_neighbors_threshold': 1000,\n",
      "  'radius': 12.0}\n",
      "Parameter annotations: (data: torch_geometric.data.batch.Batch, radius: float, max_num_neighbors_threshold: int, pbc: list[bool] = [True, True, True]).\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 414, in wrapped_fn\n",
      "    param_fn(*args, **kwargs)\n",
      "  File \"<@beartype(jmppeft.utils.radius_graph.check_params) at 0x7f6a6c73d8a0>\", line 33, in check_params\n",
      "beartype.roar.BeartypeCallHintParamViolation: Function jmppeft.utils.radius_graph.check_params() parameter data=\"Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3...')\" violates type hint <class 'torch_geometric.data.batch.Batch'>, as <protocol \"torch_geometric.data.data.Data\"> \"Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3...')\" not instance of <protocol \"torch_geometric.data.batch.Batch\">.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/ll/src/ll/runner.py\", line 411, in fast_dev_run\n",
      "    return_value = self.local([(config, *args)], env=env, reset_id=True)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ll/src/ll/runner.py\", line 193, in local\n",
      "    return_value = self._run_fn(config, *args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ll/src/ll/runner.py\", line 127, in wrapped_run\n",
      "    return run(config, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1594696/998721905.py\", line 7, in run\n",
      "    trainer.fit(model)\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/workspaces/ll/src/ll/trainer/trainer.py\", line 442, in _run\n",
      "    return super()._run(model, ckpt_path)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1032, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 138, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 204, in advance\n",
      "    batch, _, __ = next(data_fetcher)\n",
      "                   ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\", line 133, in __next__\n",
      "    batch = super().__next__()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py\", line 60, in __next__\n",
      "    batch = next(self.iterator)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 78, in __next__\n",
      "    out[i] = next(self.iterators[i])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py\", line 42, in __getitem__\n",
      "    data = self.__wrapped__.__getitem__(idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/dataset.py\", line 302, in __getitem__\n",
      "    return self.datasets[dataset_idx][sample_idx]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py\", line 42, in __getitem__\n",
      "    data = self.__wrapped__.__getitem__(idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 446, in wrapped_fn\n",
      "    out = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py\", line 119, in __getitem__\n",
      "    return self.__wrapped__.__getitem__(index % og_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py\", line 45, in __getitem__\n",
      "    data = transform(data)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 446, in wrapped_fn\n",
      "    out = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/modules/transforms/utils.py\", line 11, in composed\n",
      "    data = transform(data)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 446, in wrapped_fn\n",
      "    out = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/tasks/pretrain/module.py\", line 891, in oc22_transform\n",
      "    data = self._generate_graphs(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 446, in wrapped_fn\n",
      "    out = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/tasks/pretrain/module.py\", line 805, in _generate_graphs\n",
      "    aint_graph = generate_graph(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 446, in wrapped_fn\n",
      "    out = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/utils/goc_graph.py\", line 287, in generate_graph\n",
      "    ) = _generate_graph(\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 446, in wrapped_fn\n",
      "    out = fn(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/repositories/jmp-peft/src/jmppeft/utils/goc_graph.py\", line 226, in _generate_graph\n",
      "    edge_index, cell_offsets, neighbors = radius_graph_pbc(\n",
      "                                          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py\", line 443, in wrapped_fn\n",
      "    raise TypeCheckError(msg) from e\n",
      "jaxtyping.TypeCheckError: Type-check error whilst checking the parameters of radius_graph_pbc.\n",
      "The problem arose whilst typechecking parameter 'data'.\n",
      "Actual value: Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22')\n",
      "Expected type: <class 'torch_geometric.data.batch.Batch'>.\n",
      "----------------------\n",
      "Called with parameters: { 'data': Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22'),\n",
      "  'max_num_neighbors_threshold': 1000,\n",
      "  'radius': 12.0}\n",
      "Parameter annotations: (data: torch_geometric.data.batch.Batch, radius: float, max_num_neighbors_threshold: int, pbc: list[bool] = [True, True, True]).\n",
      "\n"
     ]
    },
    {
     "ename": "TypeCheckError",
     "evalue": "Type-check error whilst checking the parameters of radius_graph_pbc.\nThe problem arose whilst typechecking parameter 'data'.\nActual value: Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22')\nExpected type: <class 'torch_geometric.data.batch.Batch'>.\n----------------------\nCalled with parameters: { 'data': Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22'),\n  'max_num_neighbors_threshold': 1000,\n  'radius': 12.0}\nParameter annotations: (data: torch_geometric.data.batch.Batch, radius: float, max_num_neighbors_threshold: int, pbc: list[bool] = [True, True, True]).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBeartypeCallHintParamViolation\u001b[0m            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:414\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m     \u001b[43mparam_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AnnotationError:\n",
      "File \u001b[0;32m<@beartype(jmppeft.utils.radius_graph.check_params) at 0x7f6a6c73d8a0>:33\u001b[0m, in \u001b[0;36mcheck_params\u001b[0;34m(__beartype_object_94190157362304, __beartype_get_violation, __beartype_conf, __beartype_getrandbits, __beartype_func, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mBeartypeCallHintParamViolation\u001b[0m: Function jmppeft.utils.radius_graph.check_params() parameter data=\"Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3...')\" violates type hint <class 'torch_geometric.data.batch.Batch'>, as <protocol \"torch_geometric.data.data.Data\"> \"Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3...')\" not instance of <protocol \"torch_geometric.data.batch.Batch\">.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeCheckError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model)\n\u001b[1;32m      9\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(run)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_dev_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/ll/src/ll/runner.py:411\u001b[0m, in \u001b[0;36mRunner.fast_dev_run\u001b[0;34m(self, runs, env, n_batches, stop_on_error)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     config\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mfast_dev_run \u001b[38;5;241m=\u001b[39m n_batches\n\u001b[0;32m--> 411\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     return_values\u001b[38;5;241m.\u001b[39mextend(return_value)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# print full traceback\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ll/src/ll/runner.py:193\u001b[0m, in \u001b[0;36mRunner.local\u001b[0;34m(self, runs, env, reset_id)\u001b[0m\n\u001b[1;32m    191\u001b[0m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mupdate(env)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     return_values\u001b[38;5;241m.\u001b[39mappend(return_value)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ll/src/ll/runner.py:127\u001b[0m, in \u001b[0;36mRunner._run_fn.<locals>.wrapped_run\u001b[0;34m(config, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(Trainer\u001b[38;5;241m.\u001b[39mcontext(config))\n\u001b[1;32m    125\u001b[0m         log\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto-wrapping run in Trainer context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExitStack should never raise an exception\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(config, model_cls)\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model_cls(config)\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ll/src/ll/trainer/trainer.py:442\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mautomatic_optimization \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clip_algorithm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    432\u001b[0m ):\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient clipping is not supported with manual optimization. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.automatic_optimization to True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse the values in `config.trainer.gradient_clip_val` and `config.trainer.gradient_clip_algorithm`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m     )\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1032\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1032\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:138\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:204\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/lightning/pytorch/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumed[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py:42\u001b[0m, in \u001b[0;36mtransform.<locals>._TransformedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m copy_data, transform\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform must be defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy_data:\n\u001b[1;32m     44\u001b[0m     data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(data)\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/torch/utils/data/dataset.py:302\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py:42\u001b[0m, in \u001b[0;36mtransform.<locals>._TransformedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m copy_data, transform\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform must be defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy_data:\n\u001b[1;32m     44\u001b[0m     data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(data)\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:446\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py:119\u001b[0m, in \u001b[0;36mexpand_dataset.<locals>._ExpandedDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mog_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/modules/dataset/dataset_transform.py:45\u001b[0m, in \u001b[0;36mtransform.<locals>._TransformedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy_data:\n\u001b[1;32m     44\u001b[0m     data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(data)\n\u001b[0;32m---> 45\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:446\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/modules/transforms/utils.py:11\u001b[0m, in \u001b[0;36mcomposed\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomposed\u001b[39m(data: BaseData):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[0;32m---> 11\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:446\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/tasks/pretrain/module.py:891\u001b[0m, in \u001b[0;36moc22_transform\u001b[0;34m(self, data, training)\u001b[0m\n\u001b[1;32m    888\u001b[0m     data\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mfloat\u001b[39m(data\u001b[38;5;241m.\u001b[39my_relaxed))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    889\u001b[0m data\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moc22\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 891\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_graphs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcutoffs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCutoffs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMaxNeighbors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_goc_base_proportions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpbc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m data\u001b[38;5;241m.\u001b[39my_scale \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39menergy_loss_scale\n\u001b[1;32m    900\u001b[0m data\u001b[38;5;241m.\u001b[39mforce_scale \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mforce_loss_scale\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:446\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/tasks/pretrain/module.py:805\u001b[0m, in \u001b[0;36m_generate_graphs\u001b[0;34m(self, data, cutoffs, max_neighbors, pbc, training)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_graphs\u001b[39m(\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    798\u001b[0m     data: BaseData,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    803\u001b[0m     training: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    804\u001b[0m ):\n\u001b[0;32m--> 805\u001b[0m     aint_graph \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcutoffs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_neighbors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbc\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m     aint_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_aint_graph(aint_graph, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m    809\u001b[0m     subselect \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    810\u001b[0m         subselect_graph,\n\u001b[1;32m    811\u001b[0m         data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m         max_neighbors_orig\u001b[38;5;241m=\u001b[39mmax_neighbors\u001b[38;5;241m.\u001b[39maint,\n\u001b[1;32m    815\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:446\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/utils/goc_graph.py:287\u001b[0m, in \u001b[0;36mgenerate_graph\u001b[0;34m(data, cutoff, max_neighbors, pbc, symmetrize, filter_tags, sort_edges)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_graph\u001b[39m(\n\u001b[1;32m    271\u001b[0m     data: BaseData,\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m     sort_edges: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    279\u001b[0m ):\n\u001b[1;32m    280\u001b[0m     (\n\u001b[1;32m    281\u001b[0m         edge_index,\n\u001b[1;32m    282\u001b[0m         edge_dist,\n\u001b[1;32m    283\u001b[0m         distance_vec,\n\u001b[1;32m    284\u001b[0m         cell_offsets,\n\u001b[1;32m    285\u001b[0m         _,  \u001b[38;5;66;03m# cell offset distances\u001b[39;00m\n\u001b[1;32m    286\u001b[0m         num_neighbors,\n\u001b[0;32m--> 287\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_generate_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpbc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpbc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# These vectors actually point in the opposite direction.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# But we want to use col as idx_t for efficient aggregation.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     edge_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mdistance_vec \u001b[38;5;241m/\u001b[39m edge_dist[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:446\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_signature\u001b[38;5;241m.\u001b[39mreturn_annotation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mSignature\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;66;03m# Now type-check the return value. We need to include the\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# parameters in the type-checking here in case there are any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# checking of the parameters. Unfortunately there doesn't seem\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# to be a way around that, so c'est la vie.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     kwargs[output_name] \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/workspaces/repositories/jmp-peft/src/jmppeft/utils/goc_graph.py:226\u001b[0m, in \u001b[0;36m_generate_graph\u001b[0;34m(data, cutoff, max_neighbors, pbc)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_graph\u001b[39m(\n\u001b[1;32m    219\u001b[0m     data: BaseData,\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     pbc: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    224\u001b[0m ):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbc:\n\u001b[0;32m--> 226\u001b[0m         edge_index, cell_offsets, neighbors \u001b[38;5;241m=\u001b[39m \u001b[43mradius_graph_pbc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_neighbors\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m         out \u001b[38;5;241m=\u001b[39m get_pbc_distances(\n\u001b[1;32m    231\u001b[0m             data\u001b[38;5;241m.\u001b[39mpos,\n\u001b[1;32m    232\u001b[0m             edge_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m             return_distance_vec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m         )\n\u001b[1;32m    240\u001b[0m         edge_index: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/fm/lib/python3.11/site-packages/jaxtyping/_decorator.py:443\u001b[0m, in \u001b[0;36mjaxtyped.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeCheckError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Actually call the function.\u001b[39;00m\n\u001b[1;32m    446\u001b[0m out \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeCheckError\u001b[0m: Type-check error whilst checking the parameters of radius_graph_pbc.\nThe problem arose whilst typechecking parameter 'data'.\nActual value: Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22')\nExpected type: <class 'torch_geometric.data.batch.Batch'>.\n----------------------\nCalled with parameters: { 'data': Data(y=[1], pos=[136, 3], cell=[1, 3, 3], atomic_numbers=[136], natoms=136, force=[136, 3], fixed=[136], tags=[136], sid=70066, fid=54, id='21_110881', oc22=1, name='oc22'),\n  'max_num_neighbors_threshold': 1000,\n  'radius': 12.0}\nParameter annotations: (data: torch_geometric.data.batch.Batch, radius: float, max_num_neighbors_threshold: int, pbc: list[bool] = [True, True, True]).\n"
     ]
    }
   ],
   "source": [
    "from ll import Runner, Trainer\n",
    "\n",
    "\n",
    "def run(config: PretrainConfig, model_cls: type[PretrainModel]) -> None:\n",
    "    model = model_cls(config)\n",
    "    trainer = Trainer(config)\n",
    "    trainer.fit(model)\n",
    "\n",
    "runner = Runner(run)\n",
    "runner.fast_dev_run(configs, n_batches=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/nima/shared/experiment-data/lltrainer/vftqficz\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run_id = \"vftqficz\"\n",
    "run_dir = Path(\n",
    "    f\"/net/csefiles/coc-fung-cluster/nima/shared/experiment-data/lltrainer/{run_id}/\"\n",
    ")\n",
    "assert (\n",
    "    run_dir.exists() and run_dir.is_dir()\n",
    "), f\"run_dir: {run_dir} does not exist or is not a directory\"\n",
    "print(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/nima/miniforge3/envs/jmp-peft/lib/python3.11/site-packages/nshtrainer/model/config.py:1504: IdSeedWarning: BaseConfig._rng is None. The generated IDs will not be reproducible. To fix this, call BaseConfig.set_seed(...) before generating any IDs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MatbenchDiscoveryConfig(id='gd32meg8', name='mptrj', name_parts=['jmps', 'bsz40', 'linrefenergy', 'lr8e-05', 'ln', 'direct', 'maceenergy', 'maceforce', 'rele', 'ec5.0', 'fc10.0', 'sc100.0', 'posaug_std0.01'], project='jmp_mptrj', directory=DirectoryConfig(project_root=PosixPath('/net/csefiles/coc-fung-cluster/nima/shared/experiment-data')), trainer=TrainerConfig(optimizer=OptimizationConfig(log_grad_norm=True, gradient_clipping=GradientClippingConfig(value=2.0, algorithm='value')), early_stopping=EarlyStoppingConfig(patience=50, min_lr=1e-08), precision='fp16-mixed', max_epochs=500, max_time='07:00:00:00', use_distributed_sampler=False, set_float32_matmul_precision='medium'), primary_metric=MetricConfig(name='matbench_discovery/force_mae', mode='min'), meta={'jmp_kind': 's'}, train_dataset=FinetuneMPTrjHuggingfaceDatasetConfig(split='train', energy_column_mapping={'y': 'corrected_total_energy_referenced', 'y_relaxed': 'corrected_total_energy_relaxed_referenced'}), val_dataset=FinetuneMPTrjHuggingfaceDatasetConfig(split='val', energy_column_mapping={'y': 'corrected_total_energy_referenced', 'y_relaxed': 'corrected_total_energy_relaxed_referenced'}), test_dataset=FinetuneMPTrjHuggingfaceDatasetConfig(split='test', energy_column_mapping={'y': 'corrected_total_energy_referenced', 'y_relaxed': 'corrected_total_energy_relaxed_referenced'}), optimizer=AdamWConfig(lr=8e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.5, rlp=RLPConfig(patience=5, factor=0.8)), embedding=EmbeddingConfig(num_elements=120, embedding_size=256), backbone=BackboneConfig(num_spherical=7, num_radial=128, num_blocks=4, emb_size_atom=256, emb_size_edge=512, emb_size_trip_in=64, emb_size_trip_out=64, emb_size_quad_in=32, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_sbf=32, num_before_skip=2, num_after_skip=2, num_concat=1, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, direct_forces=True, sbf={'name': 'legendre_outer'}, quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, absolute_rbf_cutoff=12.0, dropout=None, edge_dropout=None, ln_per_layer=True, scale_factor_to_ln=True), batch_size=40, num_workers=7, graph_targets=[AllegroScalarTargetConfig(name='y', loss_coefficient=5.0, loss=MACEHuberEnergyLossConfig(delta=0.01), max_atomic_number=120, edge_level_energies=True), AllegroScalarTargetConfig(name='y_relaxed', loss_coefficient=2.5, loss=MACEHuberEnergyLossConfig(delta=0.01), max_atomic_number=120, edge_level_energies=True), DirectStressTargetConfig(name='stress', loss_coefficient=100.0, reduction='mean', loss=HuberLossConfig(delta=0.01))], node_targets=[NodeVectorTargetConfig(name='force', loss_coefficient=10.0, loss=MACEHuberLossConfig(delta=0.01))], parameter_specific_optimizers=[ParamSpecificOptimizerConfig(name='ln', paremeter_patterns=['backbone.h_lns.*', 'backbone.m_lns.*', 'backbone.*.scale*.ln.*'], optimizer=AdamWConfig(lr=0.00012000000000000002, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.3333333333333333, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='embedding', paremeter_patterns=['embedding.*'], optimizer=AdamWConfig(lr=2.4e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_0', paremeter_patterns=['backbone.int_blocks.0.*', 'backbone.out_blocks.1.*', 'backbone.out_blocks.0.*'], optimizer=AdamWConfig(lr=2.4e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_1', paremeter_patterns=['backbone.int_blocks.1.*', 'backbone.out_blocks.2.*'], optimizer=AdamWConfig(lr=3.2000000000000005e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_2', paremeter_patterns=['backbone.int_blocks.2.*', 'backbone.out_blocks.3.*'], optimizer=AdamWConfig(lr=4.4000000000000006e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.9090909090909091, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_3', paremeter_patterns=['backbone.int_blocks.3.*', 'backbone.out_blocks.4.*'], optimizer=AdamWConfig(lr=5e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.8, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='y.scales', paremeter_patterns=['graph_outputs._module_dict.ft_mlp_y.per_atom_scales.*', 'graph_outputs._module_dict.ft_mlp_y.per_atom_shifts.*', 'graph_outputs._module_dict.ft_mlp_y.pairwise_scales.*'], optimizer=AdamWConfig(lr=8.000000000000001e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='y_relaxed.scales', paremeter_patterns=['graph_outputs._module_dict.ft_mlp_y_relaxed.per_atom_scales.*', 'graph_outputs._module_dict.ft_mlp_y_relaxed.per_atom_shifts.*', 'graph_outputs._module_dict.ft_mlp_y_relaxed.pairwise_scales.*'], optimizer=AdamWConfig(lr=8.000000000000001e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8)))], use_balanced_batch_sampler=True, ckpt_load=CheckpointLoadConfig(checkpoint=PretrainedCheckpointConfig(path=PosixPath('/net/csefiles/coc-fung-cluster/nima/shared/checkpoints/jmp-s.pt'))), pos_noise_augmentation=PositionNoiseAugmentationConfig(noise_std=0.01, system_corrupt_prob=0.75, atom_corrupt_prob=0.5), per_graph_radius_graph=True, ignore_graph_generation_errors=False, max_neighbors=MaxNeighbors(main=25, aeaint=20, qint=8, aint=1000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_config():\n",
    "    from collections.abc import Callable\n",
    "    from pathlib import Path\n",
    "    from typing import Literal\n",
    "\n",
    "    import nshtrainer as nt\n",
    "    from jmppeft.configs.finetune.jmp_s import jmp_s_ft_config_\n",
    "    from jmppeft.modules import loss\n",
    "    from jmppeft.tasks.config import AdamWConfig\n",
    "    from jmppeft.tasks.finetune import base, output_head\n",
    "    from jmppeft.tasks.finetune import matbench_discovery as M\n",
    "    from jmppeft.utils.param_specific_util import (\n",
    "        make_parameter_specific_optimizer_config,\n",
    "        parameter_specific_optimizer_config,\n",
    "    )\n",
    "\n",
    "    jmp_s_ckpt_path = Path(\n",
    "        \"/net/csefiles/coc-fung-cluster/nima/shared/checkpoints/jmp-s.pt\"\n",
    "    )\n",
    "\n",
    "    # Set this to None if you want the run logs to be saved in the current directory\n",
    "    project_root: Path | None = Path(\n",
    "        \"/net/csefiles/coc-fung-cluster/nima/shared/experiment-data/\"\n",
    "    )\n",
    "    project_root.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def jmp_s_(config: base.FinetuneConfigBase):\n",
    "        ckpt_path = jmp_s_ckpt_path\n",
    "        assert ckpt_path.exists(), f\"Checkpoint not found: {ckpt_path}\"\n",
    "\n",
    "        jmp_s_ft_config_(config)\n",
    "        config.ckpt_load.checkpoint = base.PretrainedCheckpointConfig(\n",
    "            path=ckpt_path, ema=True\n",
    "        )\n",
    "\n",
    "        config.meta[\"jmp_kind\"] = \"s\"\n",
    "        config.name_parts.append(\"jmps\")\n",
    "\n",
    "    def parameter_specific_optimizers_(config: base.FinetuneConfigBase):\n",
    "        if config.parameter_specific_optimizers is None:\n",
    "            config.parameter_specific_optimizers = []\n",
    "\n",
    "        match config.meta[\"jmp_kind\"]:\n",
    "            case \"l\":\n",
    "                config.parameter_specific_optimizers.extend(\n",
    "                    make_parameter_specific_optimizer_config(\n",
    "                        config,\n",
    "                        config.backbone.num_blocks,\n",
    "                        {\n",
    "                            \"embedding\": 0.3,\n",
    "                            \"blocks_0\": 0.55,\n",
    "                            \"blocks_1\": 0.40,\n",
    "                            \"blocks_2\": 0.30,\n",
    "                            \"blocks_3\": 0.40,\n",
    "                            \"blocks_4\": 0.55,\n",
    "                            \"blocks_5\": 0.625,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "            case \"s\":\n",
    "                config.parameter_specific_optimizers.extend(\n",
    "                    make_parameter_specific_optimizer_config(\n",
    "                        config,\n",
    "                        config.backbone.num_blocks,\n",
    "                        {\n",
    "                            \"embedding\": 0.3,\n",
    "                            \"blocks_0\": 0.30,\n",
    "                            \"blocks_1\": 0.40,\n",
    "                            \"blocks_2\": 0.55,\n",
    "                            \"blocks_3\": 0.625,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid jmp_kind: {config.meta['jmp_kind']}\")\n",
    "\n",
    "    def parameter_specific_optimizers_energy_references_(\n",
    "        config: base.FinetuneConfigBase,\n",
    "        lr_multiplier: float = 0.1,\n",
    "    ):\n",
    "        if not config.parameter_specific_optimizers:\n",
    "            config.parameter_specific_optimizers = []\n",
    "\n",
    "        if energy_ref_heads := [\n",
    "            t\n",
    "            for t in config.graph_targets\n",
    "            if isinstance(t, output_head.ReferencedScalarTargetConfig)\n",
    "        ]:\n",
    "            config.parameter_specific_optimizers.extend(\n",
    "                parameter_specific_optimizer_config(\n",
    "                    config,\n",
    "                    [\n",
    "                        {\n",
    "                            \"name\": f\"{energy_ref_head.name}.ref\",\n",
    "                            \"lr_multiplier\": lr_multiplier,\n",
    "                            \"parameter_patterns\": [\n",
    "                                f\"graph_outputs._module_dict.ft_mlp_{energy_ref_head.name}.references.*\"\n",
    "                            ],\n",
    "                        }\n",
    "                        for energy_ref_head in energy_ref_heads\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif allegro_heads := [\n",
    "            t\n",
    "            for t in config.graph_targets\n",
    "            if isinstance(t, output_head.AllegroScalarTargetConfig)\n",
    "        ]:\n",
    "            config.parameter_specific_optimizers.extend(\n",
    "                parameter_specific_optimizer_config(\n",
    "                    config,\n",
    "                    [\n",
    "                        {\n",
    "                            \"name\": f\"{h.name}.scales\",\n",
    "                            \"lr_multiplier\": lr_multiplier,\n",
    "                            \"parameter_patterns\": [\n",
    "                                f\"graph_outputs._module_dict.ft_mlp_{h.name}.per_atom_scales.*\",\n",
    "                                f\"graph_outputs._module_dict.ft_mlp_{h.name}.per_atom_shifts.*\",\n",
    "                                *(\n",
    "                                    [\n",
    "                                        f\"graph_outputs._module_dict.ft_mlp_{h.name}.pairwise_scales.*\"\n",
    "                                    ]\n",
    "                                    if h.edge_level_energies\n",
    "                                    else []\n",
    "                                ),\n",
    "                            ],\n",
    "                        }\n",
    "                        for h in allegro_heads\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"No energy reference or allegro heads found\")\n",
    "\n",
    "    def direct_(config: base.FinetuneConfigBase):\n",
    "        config.backbone.regress_forces = True\n",
    "        config.backbone.direct_forces = True\n",
    "        config.backbone.regress_energy = True\n",
    "        config.name_parts.append(\"direct\")\n",
    "\n",
    "    def ln_(\n",
    "        config: base.FinetuneConfigBase,\n",
    "        *,\n",
    "        lr_multiplier: float | None,\n",
    "    ):\n",
    "        config.backbone.ln_per_layer = True\n",
    "        config.backbone.scale_factor_to_ln = True\n",
    "\n",
    "        if lr_multiplier is not None:\n",
    "            if config.parameter_specific_optimizers is None:\n",
    "                config.parameter_specific_optimizers = []\n",
    "\n",
    "            config.parameter_specific_optimizers = [\n",
    "                *parameter_specific_optimizer_config(\n",
    "                    config,\n",
    "                    [\n",
    "                        {\n",
    "                            \"name\": \"ln\",\n",
    "                            \"lr_multiplier\": lr_multiplier,\n",
    "                            \"parameter_patterns\": [\n",
    "                                \"backbone.h_lns.*\",\n",
    "                                \"backbone.m_lns.*\",\n",
    "                                \"backbone.*.scale*.ln.*\",\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                ),\n",
    "                *config.parameter_specific_optimizers,\n",
    "            ]\n",
    "\n",
    "        config.name_parts.append(\"ln\")\n",
    "\n",
    "    def pos_aug_(config: base.FinetuneConfigBase, *, std: float):\n",
    "        config.pos_noise_augmentation = base.PositionNoiseAugmentationConfig(\n",
    "            system_corrupt_prob=0.75,\n",
    "            atom_corrupt_prob=0.5,\n",
    "            noise_std=std,\n",
    "        )\n",
    "        config.name_parts.append(f\"posaug_std{std}\")\n",
    "\n",
    "    def data_config_(\n",
    "        config: M.MatbenchDiscoveryConfig,\n",
    "        *,\n",
    "        batch_size: int,\n",
    "        reference: bool,\n",
    "    ):\n",
    "        config.batch_size = batch_size\n",
    "        config.name_parts.append(f\"bsz{batch_size}\")\n",
    "\n",
    "        def dataset_fn(split: Literal[\"train\", \"val\", \"test\"]):\n",
    "            return base.FinetuneMPTrjHuggingfaceDatasetConfig(\n",
    "                split=split,\n",
    "                energy_column_mapping={\n",
    "                    \"y\": \"corrected_total_energy_referenced\",\n",
    "                    \"y_relaxed\": \"corrected_total_energy_relaxed_referenced\",\n",
    "                }\n",
    "                if reference\n",
    "                else {\n",
    "                    \"y\": \"corrected_total_energy\",\n",
    "                    \"y_relaxed\": \"corrected_total_energy_relaxed\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        config.train_dataset = dataset_fn(\"train\")\n",
    "        config.val_dataset = dataset_fn(\"val\")\n",
    "        config.test_dataset = dataset_fn(\"test\")\n",
    "\n",
    "        if reference:\n",
    "            config.name_parts.append(\"linrefenergy\")\n",
    "        else:\n",
    "            config.name_parts.append(\"totalenergy\")\n",
    "\n",
    "        # Set data config\n",
    "        config.num_workers = 7\n",
    "\n",
    "        # Balanced batch sampler\n",
    "        config.use_balanced_batch_sampler = True\n",
    "        config.trainer.use_distributed_sampler = False\n",
    "\n",
    "    def output_heads_config_(\n",
    "        config: M.MatbenchDiscoveryConfig,\n",
    "        *,\n",
    "        relaxed_energy: bool,\n",
    "        mace_energy_loss: bool,\n",
    "        mace_force_loss: bool,\n",
    "        energy_coefficient: float,\n",
    "        force_coefficient: float,\n",
    "        stress_coefficient: float,\n",
    "    ):\n",
    "        energy_loss = loss.HuberLossConfig(delta=0.01)\n",
    "        if mace_energy_loss:\n",
    "            energy_loss = loss.MACEHuberEnergyLossConfig(delta=0.01)\n",
    "            config.name_parts.append(\"maceenergy\")\n",
    "\n",
    "        force_loss = loss.HuberLossConfig(delta=0.01)\n",
    "        if mace_force_loss:\n",
    "            force_loss = loss.MACEHuberLossConfig(delta=0.01)\n",
    "            config.name_parts.append(\"maceforce\")\n",
    "\n",
    "        # Energy head\n",
    "        config.graph_targets.append(\n",
    "            output_head.AllegroScalarTargetConfig(\n",
    "                name=\"y\",\n",
    "                loss_coefficient=energy_coefficient,\n",
    "                loss=energy_loss.model_copy(),\n",
    "                reduction=\"sum\",\n",
    "                max_atomic_number=config.backbone.num_elements,\n",
    "                edge_level_energies=True,\n",
    "            )\n",
    "        )\n",
    "        if relaxed_energy:\n",
    "            # Relaxed Energy head\n",
    "            config.graph_targets.append(\n",
    "                output_head.AllegroScalarTargetConfig(\n",
    "                    name=\"y_relaxed\",\n",
    "                    loss_coefficient=energy_coefficient / 2.0,\n",
    "                    loss=energy_loss.model_copy(),\n",
    "                    reduction=\"sum\",\n",
    "                    max_atomic_number=config.backbone.num_elements,\n",
    "                    edge_level_energies=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            config.name_parts.append(\"rele\")\n",
    "        # Stress head\n",
    "        config.graph_targets.append(\n",
    "            output_head.DirectStressTargetConfig(\n",
    "                name=\"stress\",\n",
    "                loss_coefficient=stress_coefficient,\n",
    "                loss=loss.HuberLossConfig(delta=0.01),\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        )\n",
    "        # Force head\n",
    "        config.node_targets.append(\n",
    "            output_head.NodeVectorTargetConfig(\n",
    "                name=\"force\",\n",
    "                loss_coefficient=force_coefficient,\n",
    "                loss=force_loss,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        config.name_parts.append(f\"ec{energy_coefficient}\")\n",
    "        config.name_parts.append(f\"fc{force_coefficient}\")\n",
    "        config.name_parts.append(f\"sc{stress_coefficient}\")\n",
    "\n",
    "    def optimization_config_(\n",
    "        config: M.MatbenchDiscoveryConfig,\n",
    "        *,\n",
    "        lr: float,\n",
    "    ):\n",
    "        config.optimizer = AdamWConfig(\n",
    "            lr=lr,\n",
    "            amsgrad=False,\n",
    "            betas=(0.9, 0.95),\n",
    "            weight_decay=0.1,\n",
    "        )\n",
    "        config.lr_scheduler = base.WarmupCosRLPConfig(\n",
    "            warmup_epochs=1,\n",
    "            warmup_start_lr_factor=1.0e-1,\n",
    "            should_restart=False,\n",
    "            max_epochs=128,\n",
    "            min_lr_factor=0.5,\n",
    "            rlp=base.RLPConfig(patience=5, factor=0.8),\n",
    "        )\n",
    "        config.trainer.optimizer.gradient_clipping = nt.model.GradientClippingConfig(\n",
    "            value=2.0,\n",
    "            algorithm=\"value\",\n",
    "        )\n",
    "\n",
    "        config.name_parts.append(f\"lr{lr}\")\n",
    "\n",
    "    def create_config(config_fn: Callable[[M.MatbenchDiscoveryConfig], None]):\n",
    "        config = M.MatbenchDiscoveryConfig.draft()\n",
    "\n",
    "        config.trainer.precision = \"16-mixed-auto\"\n",
    "        config.trainer.set_float32_matmul_precision = \"medium\"\n",
    "\n",
    "        config.project = \"jmp_mptrj\"\n",
    "        config.name = \"mptrj\"\n",
    "        config_fn(config)\n",
    "        config.backbone.qint_tags = [0, 1, 2]\n",
    "\n",
    "        config.primary_metric = nt.MetricConfig(\n",
    "            name=\"matbench_discovery/force_mae\", mode=\"min\"\n",
    "        )\n",
    "\n",
    "        if project_root:\n",
    "            config.with_project_root_(project_root)\n",
    "        return config\n",
    "\n",
    "    config = create_config(jmp_s_)\n",
    "    config.parameter_specific_optimizers = []\n",
    "    config.max_neighbors = M.MaxNeighbors(main=25, aeaint=20, aint=1000, qint=8)\n",
    "    config.cutoffs = M.Cutoffs.from_constant(12.0)\n",
    "    data_config_(config, reference=True, batch_size=40)\n",
    "    optimization_config_(config, lr=8.0e-5)\n",
    "    ln_(config, lr_multiplier=1.5)\n",
    "    direct_(config=config)\n",
    "    output_heads_config_(\n",
    "        config,\n",
    "        relaxed_energy=True,\n",
    "        mace_energy_loss=True,\n",
    "        mace_force_loss=True,\n",
    "        energy_coefficient=5.0,\n",
    "        force_coefficient=10.0,\n",
    "        stress_coefficient=100.0,\n",
    "    )\n",
    "    parameter_specific_optimizers_(config)\n",
    "    parameter_specific_optimizers_energy_references_(config, lr_multiplier=0.1)\n",
    "    pos_aug_(config, std=0.01)\n",
    "    config.per_graph_radius_graph = True\n",
    "    config.ignore_graph_generation_errors = False\n",
    "\n",
    "    config = config.finalize()\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "config = make_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/nima/shared/experiment-data/lltrainer/vftqficz/log/csv/csv/mptrj-jmps-bsz16-linrefenergy-lr8e-05-ln-direct-maceenergy-maceforce-rele-ec20.0-fc20.0-sc10.0-posaug_std0.01/vftqficz/hparams.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config_updated = config\n",
    "\n",
    "hparams_file = next(run_dir.glob(\"./log/csv/csv/*/*/hparams.yaml\"))\n",
    "print(hparams_file)\n",
    "\n",
    "key_keys = (\n",
    "    \"backbone\",\n",
    "    \"embedding\",\n",
    "    \"output\",\n",
    "    \"graph_targets\",\n",
    "    \"node_targets\",\n",
    "    \"train_dataset\",\n",
    "    \"val_dataset\",\n",
    "    \"test_dataset\",\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"name_parts\",\n",
    "    # \"predict_dataset\",\n",
    ")\n",
    "\n",
    "hparams = yaml.unsafe_load(hparams_file.read_text())\n",
    "\n",
    "# Update the config with the hparams\n",
    "for key in key_keys:\n",
    "    assert (value := hparams.get(key)), f\"{key} not found in hparams\"\n",
    "\n",
    "    config_dict = config_updated.model_dump(round_trip=True)\n",
    "    config_dict[key] = value\n",
    "    config_updated = config_updated.model_validate(config_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/nima/miniforge3/envs/jmp-peft/lib/python3.11/site-packages/nshtrainer/model/config.py:1504: IdSeedWarning: BaseConfig._rng is None. The generated IDs will not be reproducible. To fix this, call BaseConfig.set_seed(...) before generating any IDs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MatbenchDiscoveryConfig(id='vftqficz', name='mptrj', name_parts=['jmps', 'bsz16', 'linrefenergy', 'lr8e-05', 'ln', 'direct', 'maceenergy', 'maceforce', 'rele', 'ec20.0', 'fc20.0', 'sc10.0', 'posaug_std0.01'], project='jmp_mptrj', directory=DirectoryConfig(project_root=PosixPath('/net/csefiles/coc-fung-cluster/nima/shared/experiment-data')), trainer=TrainerConfig(optimizer=OptimizationConfig(log_grad_norm=True, gradient_clipping=GradientClippingConfig(value=2.0, algorithm='value')), early_stopping=EarlyStoppingConfig(patience=50, min_lr=1e-08), precision='fp16-mixed', max_epochs=500, max_time='07:00:00:00', use_distributed_sampler=False, set_float32_matmul_precision='medium'), primary_metric=MetricConfig(name='matbench_discovery/force_mae', mode='min'), meta={'jmp_kind': 's'}, train_dataset=FinetuneMPTrjHuggingfaceDatasetConfig(split='train', energy_column_mapping={'y': 'corrected_total_energy_referenced', 'y_relaxed': 'corrected_total_energy_relaxed_referenced'}), val_dataset=FinetuneMPTrjHuggingfaceDatasetConfig(split='val', energy_column_mapping={'y': 'corrected_total_energy_referenced', 'y_relaxed': 'corrected_total_energy_relaxed_referenced'}), test_dataset=FinetuneMPTrjHuggingfaceDatasetConfig(split='test', energy_column_mapping={'y': 'corrected_total_energy_referenced', 'y_relaxed': 'corrected_total_energy_relaxed_referenced'}), optimizer=AdamWConfig(lr=8e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.5, rlp=RLPConfig(patience=5, factor=0.8)), embedding=EmbeddingConfig(num_elements=120, embedding_size=256), backbone=BackboneConfig(num_spherical=7, num_radial=128, num_blocks=4, emb_size_atom=256, emb_size_edge=512, emb_size_trip_in=64, emb_size_trip_out=64, emb_size_quad_in=32, emb_size_quad_out=32, emb_size_aint_in=64, emb_size_aint_out=64, emb_size_rbf=16, emb_size_cbf=16, emb_size_sbf=32, num_before_skip=2, num_after_skip=2, num_concat=1, num_atom=3, num_output_afteratom=3, num_atom_emb_layers=2, direct_forces=True, sbf={'name': 'legendre_outer'}, quad_interaction=True, atom_edge_interaction=True, edge_atom_interaction=True, atom_interaction=True, absolute_rbf_cutoff=12.0, dropout=None, edge_dropout=None, ln_per_layer=True, scale_factor_to_ln=True), batch_size=40, num_workers=7, graph_targets=[AllegroScalarTargetConfig(name='y', loss_coefficient=20.0, loss=MACEHuberEnergyLossConfig(delta=0.01), max_atomic_number=120, edge_level_energies=True), AllegroScalarTargetConfig(name='y_relaxed', loss_coefficient=10.0, loss=MACEHuberEnergyLossConfig(delta=0.01), max_atomic_number=120, edge_level_energies=True), DirectStressTargetConfig(name='stress', loss_coefficient=10.0, reduction='mean', loss=HuberLossConfig(delta=0.01))], node_targets=[NodeVectorTargetConfig(name='force', loss_coefficient=20.0, loss=MACEHuberLossConfig(delta=0.01))], parameter_specific_optimizers=[ParamSpecificOptimizerConfig(name='ln', paremeter_patterns=['backbone.h_lns.*', 'backbone.m_lns.*', 'backbone.*.scale*.ln.*'], optimizer=AdamWConfig(lr=0.00012000000000000002, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.3333333333333333, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='embedding', paremeter_patterns=['embedding.*'], optimizer=AdamWConfig(lr=2.4e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_0', paremeter_patterns=['backbone.int_blocks.0.*', 'backbone.out_blocks.1.*', 'backbone.out_blocks.0.*'], optimizer=AdamWConfig(lr=2.4e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_1', paremeter_patterns=['backbone.int_blocks.1.*', 'backbone.out_blocks.2.*'], optimizer=AdamWConfig(lr=3.2000000000000005e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_2', paremeter_patterns=['backbone.int_blocks.2.*', 'backbone.out_blocks.3.*'], optimizer=AdamWConfig(lr=4.4000000000000006e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.9090909090909091, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='blocks_3', paremeter_patterns=['backbone.int_blocks.3.*', 'backbone.out_blocks.4.*'], optimizer=AdamWConfig(lr=5e-05, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.8, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='y.scales', paremeter_patterns=['graph_outputs._module_dict.ft_mlp_y.per_atom_scales.*', 'graph_outputs._module_dict.ft_mlp_y.per_atom_shifts.*', 'graph_outputs._module_dict.ft_mlp_y.pairwise_scales.*'], optimizer=AdamWConfig(lr=8.000000000000001e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8))), ParamSpecificOptimizerConfig(name='y_relaxed.scales', paremeter_patterns=['graph_outputs._module_dict.ft_mlp_y_relaxed.per_atom_scales.*', 'graph_outputs._module_dict.ft_mlp_y_relaxed.per_atom_shifts.*', 'graph_outputs._module_dict.ft_mlp_y_relaxed.pairwise_scales.*'], optimizer=AdamWConfig(lr=8.000000000000001e-06, weight_decay=0.1, betas=(0.9, 0.95)), lr_scheduler=WarmupCosRLPConfig(warmup_epochs=1, max_epochs=128, warmup_start_lr_factor=0.1, min_lr_factor=0.99, rlp=RLPConfig(patience=5, factor=0.8)))], use_balanced_batch_sampler=True, ckpt_load=CheckpointLoadConfig(checkpoint=PretrainedCheckpointConfig(path=PosixPath('/net/csefiles/coc-fung-cluster/nima/shared/checkpoints/jmp-s.pt'))), pos_noise_augmentation=PositionNoiseAugmentationConfig(noise_std=0.01, system_corrupt_prob=0.75, atom_corrupt_prob=0.5), per_graph_radius_graph=True, ignore_graph_generation_errors=False, max_neighbors=MaxNeighbors(main=25, aeaint=20, qint=8, aint=1000))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_updated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmp-peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
